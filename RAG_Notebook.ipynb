{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5332863e",
   "metadata": {},
   "source": [
    "#Implemtentaion of RAG for a Campany Specific / domain specific chatbot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51b3be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Mint HRM\\company_chatbot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List,Dict,Any\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from langchain.document_loaders import TextLoader,PyPDFLoader,UnstructuredWordDocumentLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1123484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing ChomaDB  - store the path to your ChromaDB persist directory\n",
    "CHROMA_PERSIST_DIR = \"./chroma_db\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b2901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model name  - will download automatically if not present\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a8420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker (Cross-Encoder) \n",
    "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78387dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL LLaMA model path \n",
    "LLAMA_MODEL_PATH = r\"C:\\Users\\User\\Desktop\\Mint HRM\\company_chatbot\\model\\llama-2-7b-chat.Q3_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65dfa120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking parameters..\n",
    "CHUNK_SIZE = 800        \n",
    "CHUNK_OVERLAP = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbac7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval / rerank parameters\n",
    "TOP_K_RETRIEVE = 6\n",
    "TOP_K_RERANK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c9ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaMA generation parameters  (pass to LlamaCpp)\n",
    "LLAMA_MAX_TOKENS = 512 \n",
    "LLAMA_N_CTX = 2048\n",
    "LLAMA_N_THREADS = 8      \n",
    "LLAMA_N_GPU_LAYERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df8cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'company_documents' already exists. Using existing collection.\n"
     ]
    }
   ],
   "source": [
    "# Initializing chromadb client  ( local persistent on disk)\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)\n",
    "COLLECTION_NAME = \"company_documents\"\n",
    "\n",
    "#Create or get Collection \n",
    "\n",
    "try:\n",
    "    chroma_collection = client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists. Using existing collection.\")\n",
    "except Exception:\n",
    "    chroma_collection= client.create_collection(name=COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34934209",
   "metadata": {},
   "source": [
    "Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e8dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicate chunks of text.\n",
    "\n",
    "def compute_md5(text: str) -> str:\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9fcfaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a unique-ish ID for a file.\n",
    "\n",
    "def make_file_id(file_path: str) -> str:\n",
    "    st = Path(file_path).stat()\n",
    "    base = f\"{Path(file_path).name}-{st.st_size}-{st.st_mtime}\"\n",
    "    return hashlib.sha1(base.encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccc8b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans text before feeding it into embeddings.\n",
    "def clean_text_basic(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[\\u0000-\\u001F]+\", \" \", text)  # remove control characters\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b662cda",
   "metadata": {},
   "source": [
    "Document Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b509e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load documents from various file types\n",
    "\n",
    "def load_file_to_documents(file_path: str):\n",
    "    p=Path(file_path)\n",
    "    suffix = p.suffix.lower()\n",
    "\n",
    "    if suffix == \".txt\":\n",
    "        loader = TextLoader(str(p), encoding=\"utf-8\")\n",
    "\n",
    "    elif suffix == \".pdf\":\n",
    "        loader = PyPDFLoader(str(p))\n",
    "\n",
    "    elif suffix in [\".doc\", \".docx\"]:\n",
    "        loader = UnstructuredWordDocumentLoader(str(p))\n",
    "\n",
    "    else:\n",
    "        loader = TextLoader(str(p), encoding=\"utf-8\")\n",
    "\n",
    "    docs = loader.load()\n",
    "    print(f\"Loaded {len(docs)} document(s) from {file_path}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b473c8",
   "metadata": {},
   "source": [
    "Chunking Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c97d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking function\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "def chunk_docs_with_md5(docs:List[Any]) -> List[Dict[str,Any]]:\n",
    "   chunks=[]\n",
    "\n",
    "#Processing each document\n",
    "   for d in docs:\n",
    "      content = clean_text_basic(d.page_content)\n",
    "      if not content:\n",
    "          continue\n",
    "      pieces = text_splitter.split_text(content)\n",
    "\n",
    "\n",
    "#  Creating chunk dictionaries\n",
    "      for idx, part in enumerate(pieces):\n",
    "         md5 = compute_md5(part)\n",
    "         chunks.append({\n",
    "            \"text\": part,\n",
    "            \"md5\": md5,\n",
    "            \"source_meta\": d.metadata if hasattr(d, 'metadata') else {}\n",
    "         })\n",
    "\n",
    "   return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761346a9",
   "metadata": {},
   "source": [
    "Loading a sentence embedding model and using it to convert text into vector embeddings for  RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6832e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model ready. Vector dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# --- Load embedding model (SentenceTransformer) ---\n",
    "\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "print(\"Embedding model ready. Vector dimension:\", embedding_model.get_sentence_embedding_dimension())\n",
    "\n",
    "\n",
    "def embed_text_list(texts: List[str]) -> np.array:\n",
    "\n",
    "#\"\"\"Return numpy array of embeddings for the list of texts.\"\"\"\n",
    "    return embedding_model.encode(texts,convert_to_numpy=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6980805",
   "metadata": {},
   "source": [
    "checking for duplicates in your Chroma vector database before inserting new embeddings ---- Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf1dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existing chunk by md5 in Chroma (deduplication)\n",
    "\n",
    "def chunk_md5_exists_in_chroma(md5_value: str) -> bool:\n",
    "\n",
    " #  Try to find if any metadata with md5 == md5_value exists.\n",
    "    try:\n",
    "        res = chroma_collection.get(where={\"md5\": md5_value}, include =[\"metadatas\", \"ids\"])\n",
    "        return len(res.get(\"ids\",[]))>0\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b677511f",
   "metadata": {},
   "source": [
    "full ingestion function—it takes a file and processes it all the way from loading → chunking → deduplication → embedding → storing in Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf988c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion function\n",
    "\n",
    "def ingest_file_to_chroma(file_path:str, replace_existing:bool=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Ingest file_path into chroma collection.\n",
    "    - replace_existing: if True, we delete all chunks with same filename first.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path=str(file_path)\n",
    "    filename=Path(file_path).name\n",
    "    file_id=make_file_id(file_path)\n",
    "    uploaded_on=datetime.utcnow().isoformat()\n",
    "\n",
    "    print(f\"Processing file: {file_path} (id: {file_id})\")\n",
    "  \n",
    "\n",
    "  # Delect previous entries for same filename -- for updating files.\n",
    "\n",
    "    if replace_existing:\n",
    "        try:\n",
    "            chroma_collection.delete(where={\"filename\": filename})\n",
    "            print(f\"Deleted previous entries for filename: {filename}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Load files -> get the list of dicument object \n",
    "\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    docs = load_file_to_documents(file_path)\n",
    "    if not docs:\n",
    "        print(f\"No documents found in file: {file_path}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Chuncking + Computes MD5 for deduplication\n",
    "\n",
    "    chunks = chunk_docs_with_md5(docs)\n",
    "    print(f\"Chunks produced: {len(chunks)}\")\n",
    "\n",
    "    text_to_add = []\n",
    "    metadatas_to_add = []  \n",
    "    ids_to_add = []\n",
    "\n",
    "    for idx, c in enumerate(chunks):\n",
    "        md5_val = c[\"md5\"]\n",
    "        if chunk_md5_exists_in_chroma(md5_val):\n",
    "            print(f\"Skipping duplicate chunk {idx+1}/{len(chunks)} (md5: {md5_val})\")\n",
    "            continue\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "        text_to_add.append(c[\"text\"])\n",
    "        metadatas_to_add.append({\n",
    "            \"file_id\": file_id,\n",
    "            \"filename\": filename,\n",
    "            \"chunk_index\": idx,\n",
    "            \"md5\": md5_val,\n",
    "            \"uploaded_on\": uploaded_on,\n",
    "            \"source_meta\": str(c.get(\"source_meta\", {}))\n",
    "        })\n",
    "        ids_to_add.append(chunk_id)\n",
    "\n",
    "\n",
    "    if not text_to_add:\n",
    "        print(\"No new (non-duplicate) chunks to add.\")\n",
    "        return\n",
    "    \n",
    "     # Compute embeddings and add to Chroma\n",
    "    print(\"Computing embeddings for new chunks...\")\n",
    "    embedding_np = embed_text_list(text_to_add)\n",
    "    embedding_list = embedding_np.tolist()\n",
    "\n",
    "    print(f\"Adding {len(text_to_add)} new chunks to ChromaDB...\")\n",
    "    chroma_collection.add(\n",
    "        ids=ids_to_add,\n",
    "        documents=text_to_add,\n",
    "        metadatas=metadatas_to_add,\n",
    "        embeddings=embedding_list\n",
    "    )\n",
    "\n",
    "    print(\"Ingestion complete for file:\", file_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8732d",
   "metadata": {},
   "source": [
    "functions to delete or update documents in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "114293c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to delete documents in Chroma\n",
    "\n",
    "def delete_by_file_id(file_id: str):\n",
    "    # Delete all chunks whose metadata file_id equals given file_id.\n",
    "\n",
    "    try:\n",
    "        chroma_collection.delete(where={\"file_id\": file_id})\n",
    "        print(f\"Deleted documents with file_id: {file_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting documents with file_id {file_id}: {e}\")\n",
    "\n",
    "\n",
    "def delete_by_filename(filename: str):\n",
    "    # Delete all chunks for a given filename (useful to update a file by same name).\n",
    "\n",
    "    try:\n",
    "        chroma_collection.delete(where={\"filename\": filename})\n",
    "        print(f\"Deleted documents with filename: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting documents with filename {filename}: {e}\")\n",
    "\n",
    "def update_file_by_reupload(file_path: str):\n",
    "        \n",
    "    #Delete any previous chunks with same filename, then ingest file.\n",
    "    #Use when you replace a file but keep same filename.\n",
    "    \n",
    "    filename = Path(file_path).name\n",
    "    delete_by_filename(filename)\n",
    "    ingest_file_to_chroma(file_path, replace_existing=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84fcde",
   "metadata": {},
   "source": [
    "This block is about retrieving relevant text chunks from Chroma based on a query. It’s the search part of a RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e81fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval (Chroma query)\n",
    "\n",
    "def retrieve_candidates(query: str, top_k: int = TOP_K_RETRIEVE, metadata_filter: Dict[str, Any] = None):\n",
    "\n",
    "    if metadata_filter:\n",
    "        res = chroma_collection.query(query_texts=[query], n_results=top_k, where=metadata_filter)\n",
    "    else:\n",
    "        res = chroma_collection.query(query_texts=[query], n_results=top_k)\n",
    "\n",
    "    documents = res.get(\"documents\", [[]])[0]\n",
    "    metadatas = res.get(\"metadatas\", [[]])[0]\n",
    "    ids = res.get(\"ids\", [[]])[0]\n",
    "\n",
    "    results = []\n",
    "    for doc_text,meta, _id in zip(documents, metadatas, ids):\n",
    "        results.append({ \" document\": doc_text, \"metadata\": meta, \"id\": _id })\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31fa3b",
   "metadata": {},
   "source": [
    "This block is about optionally re-ranking your retrieved candidates using a cross-encoder, which can improve the relevance of results at the cost of extra computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d295a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reranker (Cross-Encoder) model...\n",
      "Reranker loaded.   \n"
     ]
    }
   ],
   "source": [
    "# Reranker initialization\n",
    "\n",
    "reranker = None\n",
    "\n",
    "if RERANKER_MODEL_NAME and CrossEncoder is not None:\n",
    "    try:\n",
    "      print(\"Loading reranker (Cross-Encoder) model...\")\n",
    "      reranker = CrossEncoder(RERANKER_MODEL_NAME)\n",
    "      print(\"Reranker loaded.   \")\n",
    "\n",
    "    except Exception as e:\n",
    "      print(\"Error loading reranker model:\", e)\n",
    "      reranker = None\n",
    "\n",
    "else:\n",
    "   print(\"Reranker Disabled or CrossEncoder not available.\")\n",
    "\n",
    "\n",
    "\n",
    "def rerank_candidates(query: str, candidates: List[Dict[str, Any]], top_k: int = TOP_K_RERANK):\n",
    "   \n",
    "   if reranker is None:\n",
    "        return candidates[:top_k]\n",
    "   \n",
    "   pairs = [(query, c[\" document\"]) for c in candidates]\n",
    "   scores = reranker.predict(pairs)\n",
    "   ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "   return [r[0] for r in ranked[:top_k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a9a6c",
   "metadata": {},
   "source": [
    "This block is about loading a local LLaMA model using llama-cpp-python, which lets you run the model entirely on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afdbea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from C:\\Users\\User\\Desktop\\Mint HRM\\company_chatbot\\model\\llama-2-7b-chat.Q3_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q4_K:   92 tensors\n",
      "llama_model_loader: - type q5_K:    4 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q3_K - Medium\n",
      "print_info: file size   = 3.07 GiB (3.91 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing local LLaMA model via llama-cpp-python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q3_K) (and 198 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  1283.62 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  3144.52 MiB\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_down.weight with q4_K_8x8\n",
      ".........................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 64\n",
      "llama_context: n_ubatch      = 8\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  2048 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "llama_context:        CPU compute buffer size =     3.25 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '12', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize local LLaMA via llama-cpp-python  ---\n",
    "print(\"Initializing local LLaMA model via llama-cpp-python...\")\n",
    "\n",
    "# Check that the model file exists\n",
    "if not Path(LLAMA_MODEL_PATH).exists():\n",
    "    raise FileNotFoundError(f\"LLaMA model file not found at {LLAMA_MODEL_PATH}. Please check the path.\")\n",
    "\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path = str(LLAMA_MODEL_PATH),\n",
    "    n_ctx = LLAMA_N_CTX,\n",
    "    n_threads = LLAMA_N_THREADS,\n",
    "    n_gpu_layers = LLAMA_N_GPU_LAYERS)\n",
    "\n",
    "\n",
    "\n",
    "print(\"LLaMA model loaded.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ae8e7",
   "metadata": {},
   "source": [
    "uses the local LLaMA model to generate an answer based on the retrieved context chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e87d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate answer with LLaMA using retrieved context ---\n",
    "\n",
    "def generate_grounded_answer(user_query: str, contexts: List[Dict[str, Any]], max_tokens: int = LLAMA_MAX_TOKENS):\n",
    "    \n",
    "    #Build a prompt that contains retrieved contexts and ask the local LLaMA model to answer using ONLY that context.\n",
    "\n",
    "    ctx_texts = []\n",
    "\n",
    "    for c in contexts:\n",
    "        fname = c[\"metadata\"].get(\"filename\", \"unknown\")\n",
    "        idx = c[\"metadata\"].get(\"chunk_index\", -1)\n",
    "\n",
    "        chunk_text = c[\" document\"]\n",
    "        ctx_texts.append(f\"[{fname} - chunk {idx}]: {chunk_text}\")\n",
    "\n",
    "    context_block = \"\\n\\n\".join(ctx_texts)\n",
    "\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a helpful and polite assistant. Read the context below carefully and answer the question based ONLY on that information. \"\n",
    "        \"If the answer is not contained in the context, kindly respond that you do not know, without making assumptions.\\n\\n\"\n",
    "        f\"CONTEXT:\\n{context_block}\\n\\n\"\n",
    "        f\"QUESTION: {user_query}\\n\\n\"\n",
    "        \"ANSWER (use the context, be clear and concise, and be polite if unsure):\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # call local LLaMA model to generate answer\n",
    "    resp = llm(prompt, max_tokens=max_tokens)\n",
    "    return resp\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82abd8",
   "metadata": {},
   "source": [
    "This block is the final “all-in-one” function that ties together retrieval, reranking, and answer generation. It’s what you would call for any user question in your RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17b0d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- End-to-end query function: retrieve -> rerank -> generate ---\n",
    "\n",
    "def answer_user_query(user_query: str, restrict_metadata: Dict[str, Any] = None, retrieve_k: int= TOP_K_RETRIEVE, user_rerank: bool = True):\n",
    "\n",
    "    # 1 -- retrieve candidate \n",
    "    candidates = retrieve_candidates(user_query, top_k=retrieve_k, metadata_filter=restrict_metadata)   \n",
    "    if not candidates:\n",
    "        return \"No relevant documents found in the database.\"\n",
    "    \n",
    "    # 2 -- rerank \n",
    "    if user_rerank:\n",
    "        top_candidates = rerank_candidates(user_query, candidates, top_k=min(TOP_K_RERANK, len(candidates)))\n",
    "    else:\n",
    "        top_candidates = candidates[:min(TOP_K_RERANK, len(candidates))]\n",
    "\n",
    "    # 3 -- generate final answer\n",
    "    answer_text = generate_grounded_answer(user_query, top_candidates)\n",
    "    return answer_text\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1ba4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Welcome to Your RAG Assistant ---\n",
      "Options:\n",
      "1) Upload documents into vector DB\n",
      "2) Directly chat with AI assistant\n",
      "\n",
      "You selected option 2.\n",
      "\n",
      "💬 Starting chat without uploading documents...\n",
      "\n",
      "--- Chat Mode ---\n",
      "Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 55 prefix-match hit, remaining 729 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   24747.36 ms\n",
      "llama_perf_context_print: prompt eval time =   77768.93 ms /   729 tokens (  106.68 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24035.54 ms /   146 runs   (  164.63 ms per token,     6.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  101958.33 ms /   875 tokens\n",
      "llama_perf_context_print:    graphs reused =        208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "I'm glad you're interested in learning more about the book \"Rich Dad Poor Dad\"! Based on the provided context, the book appears to be a personal development and financial advice book written by Robert T. Kiyosaki. The book explores the author's experiences and perspectives on money, investing, and wealth-building, with the aim of teaching readers how to become financially independent. The book covers topics such as the importance of financial education, the differences between assets and liabilities, and the benefits of investing in real estate and other assets. I hope this information helps! If you have any further questions or need clarification, please feel free to ask.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 70 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   24747.36 ms\n",
      "llama_perf_context_print: prompt eval time =   12368.13 ms /   199 tokens (   62.15 ms per token,    16.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3511.18 ms /    32 runs   (  109.72 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   15901.76 ms /   231 tokens\n",
      "llama_perf_context_print:    graphs reused =         47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  I apologize, but the information you have provided does not contain the name of the prime minister of Sri Lanka. Therefore, I cannot answer your question.\n",
      "\n",
      "👋 Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"MAIN INTERACTIVE MENU\n",
    "Choose whether to upload documents first or directly start chatting.\n",
    "\"\"\"\n",
    "\n",
    "def run_menu():\n",
    "    print(\"\\n--- Welcome to Your RAG Assistant ---\")\n",
    "    print(\"Options:\")\n",
    "    print(\"1) Upload documents into vector DB\")\n",
    "    print(\"2) Directly chat with AI assistant\")\n",
    "    choice = input(\"Enter your choice (1/2): \").strip()\n",
    "    print(f\"\\nYou selected option {choice}.\")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        print(\"\\n📂 Please place your documents inside the 'DATA/' folder.\")\n",
    "        UPLOAD_DIR = './DATA'\n",
    "        Path(UPLOAD_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "        files = list(Path(UPLOAD_DIR).glob('*'))\n",
    "        if not files:\n",
    "            print(\"⚠️ No files found in DATA/. Add some documents and run again.\")\n",
    "        else:\n",
    "            for f in tqdm(files, desc=\"Ingesting files\"):\n",
    "                ingest_file_to_chroma(str(f), replace_existing=False)\n",
    "            print(\"\\n✅ All files processed and stored in ChromaDB.\")\n",
    "\n",
    "        # After ingestion, switch to chat\n",
    "        start_chat_loop()\n",
    "\n",
    "    elif choice == \"2\":\n",
    "        print(\"\\n💬 Starting chat without uploading documents...\")\n",
    "        start_chat_loop()\n",
    "    else:\n",
    "        print(\"❌ Invalid choice. Please restart and enter 1 or 2.\")\n",
    "\n",
    "\n",
    "def start_chat_loop():\n",
    "    print(\"\\n--- Chat Mode ---\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"👋 Goodbye!\")\n",
    "            break\n",
    "        answer = answer_user_query(query)\n",
    "        print(f\"AI: {answer}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_menu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa5da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29554214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d231e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
